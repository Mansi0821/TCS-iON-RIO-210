{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mansi0821/TCS-iON-RIO-210/blob/main/SemanticsErrorDetection_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92583a31",
      "metadata": {
        "id": "92583a31"
      },
      "outputs": [],
      "source": [
        "!pip install happytransformer \n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a1fc631",
      "metadata": {
        "id": "2a1fc631"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from datasets import load_dataset\n",
        "from happytransformer import TTSettings\n",
        "from happytransformer import TTTrainArgs\n",
        "from happytransformer import HappyTextToText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d5fcef",
      "metadata": {
        "id": "68d5fcef",
        "outputId": "f38b7c45-8506-4af5-bb6e-ee72bc8e986b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Mansi Dokrimare\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you you tooo you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "05/29/2023 12:39:33 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
          ]
        }
      ],
      "source": [
        "happy_tt = HappyTextToText(\"T5\", \"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc347a77",
      "metadata": {
        "id": "bc347a77"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"jfleg\", split='validation[:]')\n",
        "\n",
        "eval_dataset = load_dataset(\"jfleg\", split='test[:]')\n",
        "\n",
        "#MAnsi Firdst push\n",
        "#Merging MansiLocalBrnach with Main branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82555bc5",
      "metadata": {
        "id": "82555bc5",
        "outputId": "418067ba-aaee-4e46-f880-ed438ec58c77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['So I think we would not be alive if our ancestors did not develop sciences and technologies . ', 'So I think we could not live if older people did not develop science and technologies . ', 'So I think we can not live if old people could not find science and technologies and they did not develop . ', 'So I think we can not live if old people can not find the science and technology that has not been developed . ']\n",
            "So I think we would not be alive if our ancestors did not develop sciences and technologies . \n",
            "--------------------------------------------------------\n",
            "['Not for use with a car . ', 'Do not use in the car . ', 'Car not for use . ', 'Can not use the car . ']\n",
            "Not for use with a car . \n",
            "--------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for case in train_dataset[\"corrections\"][:2]:\n",
        "    print(case)\n",
        "    print(case[0])\n",
        "    print(\"--------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80713183",
      "metadata": {
        "id": "80713183"
      },
      "outputs": [],
      "source": [
        "def generate_csv(csv_path, dataset):\n",
        "    with open(csv_path, 'w', newline='') as csvfile:\n",
        "        writter = csv.writer(csvfile)\n",
        "        writter.writerow([\"input\", \"target\"])\n",
        "        for case in dataset:\n",
        "     \t    # Adding the task's prefix to input \n",
        "            input_text = \"grammar: \" + case[\"sentence\"]\n",
        "            for correction in case[\"corrections\"]:\n",
        "                # a few of the cases contain blank strings. \n",
        "                if input_text and correction:\n",
        "                    writter.writerow([input_text, correction])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4ac617f",
      "metadata": {
        "id": "b4ac617f"
      },
      "outputs": [],
      "source": [
        "generate_csv(\"train.csv\", train_dataset)\n",
        "generate_csv(\"eval.csv\", eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c1c163e",
      "metadata": {
        "id": "4c1c163e",
        "outputId": "79a8cbea-fd46-4144-b31b-0cf5c4ed2f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (0.19.0)Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from accelerate) (1.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from accelerate) (1.21.5)\n",
            "Requirement already satisfied: psutil in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (4.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "658c5245",
      "metadata": {
        "id": "658c5245",
        "outputId": "75e258c8-3a12-4624-adb8-6d927ca58e17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.29.2\n",
            "Uninstalling transformers-4.29.2:\n",
            "  Successfully uninstalled transformers-4.29.2\n",
            "Found existing installation: accelerate 0.19.0\n",
            "Uninstalling accelerate-0.19.0:\n",
            "  Successfully uninstalled accelerate-0.19.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip uninstall -y transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b23ca104",
      "metadata": {
        "id": "b23ca104",
        "outputId": "c1f1cd65-f3a5-4d57-cfae-5e6088bcbe37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from accelerate) (1.7.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.3.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mansi dokrimare\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
            "Installing collected packages: accelerate, transformers\n",
            "Successfully installed accelerate-0.19.0 transformers-4.29.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9094dc07",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7879603569d544bfb6079680fd5ab18b",
            "72bb0eb0a68b4da7b31b0440cbb053fd",
            "",
            "824355b7e3cb48c080e6a9b2adaa1470"
          ]
        },
        "id": "9094dc07",
        "outputId": "544f7dc6-41ea-45e1-a080-422c8ed0d72e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "05/29/2023 13:13:35 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/default to C:/Users/Mansi Dokrimare/.cache/huggingface/datasets/csv/default-390892e0ade50fe6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7879603569d544bfb6079680fd5ab18b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72bb0eb0a68b4da7b31b0440cbb053fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating eval split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to C:/Users/Mansi Dokrimare/.cache/huggingface/datasets/csv/default-390892e0ade50fe6/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "824355b7e3cb48c080e6a9b2adaa1470",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2988 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Mansi Dokrimare\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3606: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2988' max='2988' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2988/2988 16:27]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "before_result = happy_tt.eval(\"eval.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361b7f0a",
      "metadata": {
        "id": "361b7f0a",
        "outputId": "9ee4a22c-1c71-4f08-8b18-70e2adc88ecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before loss: 1.28038489818573\n"
          ]
        }
      ],
      "source": [
        "print(\"Before loss:\", before_result.loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "813ef53c",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "dca2468b545e4771a85a548b70f41e73",
            "4827c18f01ba49f2bd396f9137db1bef",
            "",
            "db1eb388bd224d3c93ec308dfa02d275"
          ]
        },
        "id": "813ef53c",
        "outputId": "5572ebc3-c98d-4995-c4ed-d8ba8172f26e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "05/29/2023 13:38:30 - INFO - happytransformer.happy_transformer -   Preprocessing training data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/default to C:/Users/Mansi Dokrimare/.cache/huggingface/datasets/csv/default-657a86e7a03370d3/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dca2468b545e4771a85a548b70f41e73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4827c18f01ba49f2bd396f9137db1bef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to C:/Users/Mansi Dokrimare/.cache/huggingface/datasets/csv/default-657a86e7a03370d3/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db1eb388bd224d3c93ec308dfa02d275",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3016 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Mansi Dokrimare\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3606: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "05/29/2023 13:38:39 - INFO - happytransformer.happy_transformer -   Training...\n",
            "C:\\Users\\Mansi Dokrimare\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='342' max='1131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 342/1131 44:15 < 1:42:43, 0.13 it/s, Epoch 0.90/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "args = TTTrainArgs(batch_size=8)\n",
        "happy_tt.train(\"train.csv\", args=args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6553b4",
      "metadata": {
        "id": "ce6553b4"
      },
      "outputs": [],
      "source": [
        "before_loss = happy_tt.eval(\"eval.csv\")\n",
        "\n",
        "print(\"After loss: \", before_loss.loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6d2e1e",
      "metadata": {
        "id": "ca6d2e1e"
      },
      "outputs": [],
      "source": [
        "beam_settings =  TTSettings(num_beams=5, min_length=1, max_length=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "998a3bd6",
      "metadata": {
        "id": "998a3bd6",
        "outputId": "3141e8ad-66a5-466a-fb00-1d6ceb18e0db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This sentence has bad grammar and spelling!\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Example1: \"\"\"\n",
        "example_1 = \"grammar: This sentences, has bads grammar and spelling!\"\n",
        "result_1 = happy_tt.generate_text(example_1, args=beam_settings)\n",
        "print(result_1.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6579fa3",
      "metadata": {
        "id": "d6579fa3",
        "outputId": "20aa38a4-7488-4687-daec-9d659dce8611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I enjoy writing articles on AI and I also enjoy writing articles on AI .\n"
          ]
        }
      ],
      "source": [
        "example_2 = \"grammar: I am enjoys, writtings articles ons AI and I also enjoyed write articling on AI.\"\n",
        "\n",
        "result_2 = happy_tt.generate_text(example_2, args=beam_settings)\n",
        "print(result_2.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00969417",
      "metadata": {
        "id": "00969417"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
